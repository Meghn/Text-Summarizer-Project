# Step 6.1: Step 1 in the workflow. Configuration required to run some files
# Step 7.1
# Changes made here would be reflected in all the files this is called. So it reduces the hassle of manually going to each file and changing.
artifacts_root: artifacts

data_ingestion:
  root_dir: artifacts/data_ingestion # creates data_ingestion in artifacts
  source_URL: https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip # Gets from this URL
  local_data_file: artifacts/data_ingestion/data.zip # Downloads the data in the URL as this file
  unzip_dir: artifacts/data_ingestion # When we unzip data where do we store it

data_validation:
  root_dir: artifacts/data_validation # creates data_validation in artifacts
  STATUS_FILE: artifacts/data_validation/status.txt # If all three are there status will be true else false
  ALL_REQUIRED_FILES: ["train", "test", "validation"] # Checks if all three are available

data_transformation:
  root_dir: artifacts/data_transformation # creates data_transformation in artifacts
  data_path: artifacts/data_ingestion/samsum_dataset # the dataset we wants to work on
  tokenizer_name: google/pegasus-cnn_dailymail # Model for Tokenization

model_trainer:
  root_dir: artifacts/model_trainer # creates model_trainer in artifacts
  data_path: artifacts/data_transformation/samsum_dataset
  model_ckpt: google/pegasus-cnn_dailymail # Pre-trained model

model_evaluation:
  root_dir: artifacts/model_evaluation
  data_path: artifacts/data_transformation/samsum_dataset
  model_path: artifacts/model_trainer/pegasus-samsum-model
  tokenizer_path: artifacts/model_trainer/tokenizer
  metric_file_name: artifacts/model_evaluation/metrics.csv